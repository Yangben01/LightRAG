### 这是 .env 文件的示例文件

###########################
### 服务器配置
###########################
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='GEOGraph KB'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"
# WORKERS=2
### gunicorn worker 超时时间（如果未设置 LLM_TIMEOUT，则作为默认的 LLM 请求超时时间）
# TIMEOUT=150
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### 可选的 SSL 配置
# SSL=true
# SSL_CERTFILE=/path/to/cert.pem
# SSL_KEYFILE=/path/to/key.pem

### 目录配置（默认为当前工作目录）
### 默认值为 ./inputs 和 ./rag_storage
# INPUT_DIR=<absolute_path_for_doc_input_dir>
# WORKING_DIR=<absolute_path_for_working_dir>

### Tiktoken 缓存目录（在此文件夹中存储缓存文件以供离线部署）
# TIKTOKEN_CACHE_DIR=/app/data/tiktoken

### Ollama 模拟模型和标签
# OLLAMA_EMULATING_MODEL_NAME=lightrag
OLLAMA_EMULATING_MODEL_TAG=latest

### 图检索的最大节点数（确保 WebUI 本地设置也已更新，限制为此值）
# MAX_GRAPH_NODES=1000

### 日志级别
# LOG_LEVEL=INFO
# VERBOSE=False
# LOG_MAX_BYTES=10485760
# LOG_BACKUP_COUNT=5
### 日志文件位置（默认为当前工作目录）
# LOG_DIR=/path/to/log/directory

#####################################
### 登录和 API 密钥配置
#####################################
# AUTH_ACCOUNTS='admin:admin123,user1:pass456'
# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server
# TOKEN_EXPIRE_HOURS=48
# GUEST_TOKEN_EXPIRE_HOURS=24
# JWT_ALGORITHM=HS256

### 访问 LightRAG 服务器 API 的 API 密钥
### 在 HTTP 请求中使用 'X-API-Key' 头部来使用此密钥
### 示例: curl -H "X-API-Key: your-secure-api-key-here" http://localhost:9621/query
# LIGHTRAG_API_KEY=your-secure-api-key-here
# WHITELIST_PATHS=/health,/api/*

######################################################################################
### 查询配置
###
### 如何控制发送到 LLM 的上下文长度：
###    MAX_ENTITY_TOKENS + MAX_RELATION_TOKENS < MAX_TOTAL_TOKENS
###    Chunk_Tokens = MAX_TOTAL_TOKENS - Actual_Entity_Tokens - Actual_Relation_Tokens
######################################################################################
### 查询的 LLM 响应缓存（对流式响应无效）
ENABLE_LLM_CACHE=true
# COSINE_THRESHOLD=0.2
### 从知识图谱中检索的实体或关系数量
# TOP_K=40
### 朴素向量搜索的最大块数
# CHUNK_TOP_K=20
### 控制发送到 LLM 的实际实体数量
# MAX_ENTITY_TOKENS=6000
### 控制发送到 LLM 的实际关系数量
# MAX_RELATION_TOKENS=8000
### 控制发送到 LLM 的最大 token 数（包括实体、关系和块）
# MAX_TOTAL_TOKENS=30000

### 块选择策略
###     VECTOR: 通过向量相似度选择 KG 块，交付给 LLM 的块更接近朴素检索
###     WEIGHT: 通过实体和块权重选择 KG 块，交付更多纯 KG 相关的块给 LLM
###     如果启用重排序，块选择策略的影响将减弱。
# KG_CHUNK_PICK_METHOD=VECTOR

#########################################################
### 重排序配置
### RERANK_BINDING 类型: null, cohere, jina, aliyun
### 对于使用 vLLM 部署的 rerank 模型，使用 cohere binding
#########################################################
RERANK_BINDING=aliyun
### 当 RERANK_BINDING 不为 null 时，在查询参数中默认启用 rerank
# RERANK_BY_DEFAULT=True
### rerank 分数块过滤器（设置为 0.0 保留所有块，如果 LLM 不够强大则设置为 0.6 或更高）
# MIN_RERANK_SCORE=0.0

### 使用 vLLM 本地部署
# RERANK_MODEL=BAAI/bge-reranker-v2-m3
# RERANK_BINDING_HOST=http://localhost:8000/v1/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

### Cohere AI 的默认值
# RERANK_MODEL=rerank-v3.5
# RERANK_BINDING_HOST=https://api.cohere.com/v2/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here
### Cohere rerank 分块配置（对于有 token 限制的模型如 ColBERT 很有用）
# RERANK_ENABLE_CHUNKING=true
# RERANK_MAX_TOKENS_PER_DOC=480

### Jina AI 的默认值
# RERANK_MODEL=jina-reranker-v2-base-multilingual
# RERANK_BINDING_HOST=https://api.jina.ai/v1/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

### 阿里云的默认值
RERANK_MODEL=gte-rerank-v2
RERANK_BINDING_HOST=https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank
RERANK_BINDING_API_KEY=sk-3a21e20e146549b0ad7770297ceaa1b9

########################################
### 文档处理配置
########################################
ENABLE_LLM_CACHE_FOR_EXTRACT=true

### 文档处理输出语言: English, Chinese, French, German ...
SUMMARY_LANGUAGE=Chinese

### 受保护 PDF 文件的解密密码
# PDF_DECRYPT_PASSWORD=your_pdf_password_here

### LLM 将尝试识别的实体类型
# ENTITY_TYPES='["Person", "Creature", "Organization", "Location", "Event", "Concept", "Method", "Content", "Data", "Artifact", "NaturalObject"]'

### 文档分割的块大小，建议 500~1500
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100

### 触发 LLM 对实体/关系合并进行摘要的摘要段数或 token 数（建议至少为 3）
# FORCE_LLM_SUMMARY_ON_MERGE=8
### 触发 LLM 摘要的最大描述 token 大小
# SUMMARY_MAX_TOKENS = 1200
### 建议的 LLM 摘要输出长度（以 token 为单位）
# SUMMARY_LENGTH_RECOMMENDED_=600
### 发送到 LLM 进行描述摘要的最大上下文大小
# SUMMARY_CONTEXT_SIZE=12000

### 控制向量和图数据库中存储的最大 chunk_ids 数量
# MAX_SOURCE_IDS_PER_ENTITY=300
# MAX_SOURCE_IDS_PER_RELATION=300
### 控制 chunk_ids 限制方法: FIFO, KEEP
###    FIFO: 先进先出
###    KEEP: 保留最旧的（更少的合并操作且更快）
# SOURCE_IDS_LIMIT_METHOD=FIFO

### 存储在实体/关系 file_path 字段中的最大文件路径数（仅用于显示，不影响查询性能）
# MAX_FILE_PATHS=100

### 每个源实体或关系的最大相关块数
###     块选择器使用此值来确定从 KG（知识图谱）中选择的块总数
###     较高的值会增加重排序时间
# RELATED_CHUNK_NUMBER=5

###############################
### 并发配置
###############################
### LLM 的最大并发请求数（用于查询和文档处理）
MAX_ASYNC=4
### 并行处理文档的数量（2~10 之间，建议为 MAX_ASYNC/3）
MAX_PARALLEL_INSERT=2
### Embedding 的最大并发请求数
# EMBEDDING_FUNC_MAX_ASYNC=8
### 单个请求中发送到 Embedding 的块数
# EMBEDDING_BATCH_NUM=10

###########################################################################
### LLM 配置
### LLM_BINDING 类型: openai, ollama, lollms, azure_openai, aws_bedrock, gemini
### LLM_BINDING_HOST: 仅用于 Ollama 的主机，其他 LLM 服务的端点
### 如果 LightRAG 部署在 Docker 中：
###    在 LLM_BINDING_HOST 中使用 host.docker.internal 而不是 localhost
###########################################################################
### 所有 LLM 的请求超时设置（0 表示 Ollama 无超时）
# LLM_TIMEOUT=180

LLM_BINDING=openai
LLM_MODEL=qwen-plus
LLM_BINDING_HOST=https://dashscope.aliyuncs.com/compatible-mode/v1
LLM_BINDING_API_KEY=sk-32d3ad2bb72c481ea56e1657af8f10f0

### Azure OpenAI 示例
### 使用部署名称作为模型名称或设置 AZURE_OPENAI_DEPLOYMENT
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
# LLM_BINDING=azure_openai
# LLM_BINDING_HOST=https://xxxx.openai.azure.com/
# LLM_BINDING_API_KEY=your_api_key
# LLM_MODEL=my-gpt-mini-deployment

### Openrouter 示例
# LLM_MODEL=google/gemini-2.5-flash
# LLM_BINDING_HOST=https://openrouter.ai/api/v1
# LLM_BINDING_API_KEY=your_api_key
# LLM_BINDING=openai

### Gemini 示例
# LLM_BINDING=gemini
# LLM_MODEL=gemini-flash-latest
# LLM_BINDING_API_KEY=your_gemini_api_key
# LLM_BINDING_HOST=https://generativelanguage.googleapis.com

### 使用以下命令查看 OpenAI、azure_openai 或 OpenRouter 的所有支持选项
### lightrag-server --llm-binding gemini --help
### Gemini 特定参数
# GEMINI_LLM_MAX_OUTPUT_TOKENS=9000
# GEMINI_LLM_TEMPERATURE=0.7
### 启用思考模式
# GEMINI_LLM_THINKING_CONFIG='{"thinking_budget": -1, "include_thoughts": true}'
### 禁用思考模式
# GEMINI_LLM_THINKING_CONFIG='{"thinking_budget": 0, "include_thoughts": false}'

### 使用以下命令查看 OpenAI、azure_openai 或 OpenRouter 的所有支持选项
### lightrag-server --llm-binding openai --help
### OpenAI 特定参数
# OPENAI_LLM_REASONING_EFFORT=minimal
### OpenRouter 特定参数
# OPENAI_LLM_EXTRA_BODY='{"reasoning": {"enabled": false}}'
### 通过 vLLM 部署的 Qwen3 特定参数
# OPENAI_LLM_EXTRA_BODY='{"chat_template_kwargs": {"enable_thinking": false}}'

### OpenAI 兼容 API 特定参数
### 增加温度值可能会缓解某些 LLM（如 Qwen3-30B）中的无限推理循环。
# OPENAI_LLM_TEMPERATURE=0.9
### 设置 max_tokens 以缓解某些 LLM 的无限输出（小于 LLM_TIMEOUT * llm_output_tokens/second，即 9000 = 180s * 50 tokens/s）
### 通常，max_tokens 不包括提示内容
### 对于 vLLM/SGLang 部署的模型，或大多数 OpenAI 兼容 API 提供商
# OPENAI_LLM_MAX_TOKENS=9000
### 对于 OpenAI o1-mini 或更新的模型，使用 max_completion_tokens 而不是 max_tokens
OPENAI_LLM_MAX_COMPLETION_TOKENS=9000

### 使用以下命令查看 Ollama LLM 的所有支持选项
### lightrag-server --llm-binding ollama --help
### Ollama 服务器特定参数
### OLLAMA_LLM_NUM_CTX 必须提供，并且应至少大于 MAX_TOTAL_TOKENS + 2000
OLLAMA_LLM_NUM_CTX=32768
### 设置 max_output_tokens 以缓解某些 LLM 的无限输出（小于 LLM_TIMEOUT * llm_output_tokens/second，即 9000 = 180s * 50 tokens/s）
# OLLAMA_LLM_NUM_PREDICT=9000
### Ollama LLM 的停止序列
# OLLAMA_LLM_STOP='["</s>", "<|EOT|>"]'

### Bedrock 特定参数
# BEDROCK_LLM_TEMPERATURE=1.0

#######################################################################################
### Embedding 配置（在处理第一个文件后不应更改）
### EMBEDDING_BINDING: ollama, openai, azure_openai, jina, lollms, aws_bedrock
### EMBEDDING_BINDING_HOST: 仅用于 Ollama 的主机，其他 Embedding 服务的端点
### 如果 LightRAG 部署在 Docker 中：
###    在 EMBEDDING_BINDING_HOST 中使用 host.docker.internal 而不是 localhost
#######################################################################################
# EMBEDDING_TIMEOUT=30

### 控制是否向 embedding API 发送 embedding_dim 参数
### 重要：Jina 始终发送维度参数（API 要求）- 此设置对 Jina 无效
### 对于 OpenAI：设置为 'true' 以启用动态维度调整
### 对于 OpenAI：设置为 'false'（默认）以禁用发送维度参数
### 注意：对于不支持维度参数的后端（例如 Ollama），自动忽略

### Ollama embedding
# EMBEDDING_BINDING=ollama
# EMBEDDING_MODEL=bge-m3:latest
# EMBEDDING_DIM=1024
# EMBEDDING_BINDING_API_KEY=your_api_key
### 如果 LightRAG 部署在 Docker 中，使用 host.docker.internal 而不是 localhost
# EMBEDDING_BINDING_HOST=http://localhost:11434

### OpenAI 兼容 embedding
EMBEDDING_BINDING=openai
EMBEDDING_MODEL=text-embedding-v4
EMBEDDING_DIM=1024
EMBEDDING_SEND_DIM=false
EMBEDDING_TOKEN_LIMIT=8192
EMBEDDING_BINDING_HOST=https://dashscope.aliyuncs.com/compatible-mode/v1
EMBEDDING_BINDING_API_KEY=sk-3a21e20e146549b0ad7770297ceaa1b9

### Azure embedding 的可选配置
### 使用部署名称作为模型名称或设置 AZURE_EMBEDDING_DEPLOYMENT
# AZURE_EMBEDDING_API_VERSION=2024-08-01-preview
# EMBEDDING_BINDING=azure_openai
# EMBEDDING_BINDING_HOST=https://xxxx.openai.azure.com/
# EMBEDDING_API_KEY=your_api_key
# EMBEDDING_MODEL==my-text-embedding-3-large-deployment
# EMBEDDING_DIM=3072

### Gemini embedding
# EMBEDDING_BINDING=gemini
# EMBEDDING_MODEL=gemini-embedding-001
# EMBEDDING_DIM=1536
# EMBEDDING_TOKEN_LIMIT=2048
# EMBEDDING_BINDING_HOST=https://generativelanguage.googleapis.com
# EMBEDDING_BINDING_API_KEY=your_api_key
### Gemini embedding 需要向服务器发送维度
# EMBEDDING_SEND_DIM=true

### Jina AI Embedding
# EMBEDDING_BINDING=jina
# EMBEDDING_BINDING_HOST=https://api.jina.ai/v1/embeddings
# EMBEDDING_MODEL=jina-embeddings-v4
# EMBEDDING_DIM=2048
# EMBEDDING_BINDING_API_KEY=your_api_key

### Ollama embedding 的可选配置
OLLAMA_EMBEDDING_NUM_CTX=8192
### 使用以下命令查看 Ollama embedding 的所有支持选项
### lightrag-server --embedding-binding ollama --help

####################################################################
### WORKSPACE 为所有存储类型设置工作空间名称
### 用于隔离 LightRAG 实例的数据。
### 有效的工作空间名称约束: a-z, A-Z, 0-9, 和 _
####################################################################
WORKSPACE=space1

############################
### 数据存储选择
############################
### 默认存储（推荐用于小规模部署）
# LIGHTRAG_KV_STORAGE=JsonKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=NetworkXStorage
# LIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage

### Redis 存储（推荐用于生产部署）
# LIGHTRAG_KV_STORAGE=RedisKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=RedisDocStatusStorage

### 向量存储（推荐用于生产部署）
# LIGHTRAG_VECTOR_STORAGE=MilvusVectorDBStorage
# LIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage
# LIGHTRAG_VECTOR_STORAGE=FaissVectorDBStorage

### 图存储（推荐用于生产部署）
LIGHTRAG_GRAPH_STORAGE=Neo4JStorage

### PostgreSQL
LIGHTRAG_KV_STORAGE=PGKVStorage
LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=PGGraphStorage
LIGHTRAG_VECTOR_STORAGE=PGVectorStorage

### MongoDB（向量存储仅在 Atlas Cloud 上可用）
# LIGHTRAG_KV_STORAGE=MongoKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=MongoDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=MongoGraphStorage
# LIGHTRAG_VECTOR_STORAGE=MongoVectorDBStorage

### PostgreSQL 配置 host.docker.internal 
POSTGRES_HOST=host.docker.internal 
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD='postgres'
POSTGRES_DATABASE=geo_saas
POSTGRES_MAX_CONNECTIONS=12
### 不应设置特定于数据库的工作空间，仅保留以兼容
### POSTGRES_WORKSPACE=forced_workspace_name

### PostgreSQL 向量存储配置
### 向量存储类型: HNSW, IVFFlat, VCHORDRQ
POSTGRES_VECTOR_INDEX_TYPE=HNSW
POSTGRES_HNSW_M=16
POSTGRES_HNSW_EF=200
POSTGRES_IVFFLAT_LISTS=100
POSTGRES_VCHORDRQ_BUILD_OPTIONS=
POSTGRES_VCHORDRQ_PROBES=
POSTGRES_VCHORDRQ_EPSILON=1.9

### PostgreSQL 连接重试配置（网络健壮性）
### 重试次数（1-10，默认：3）
### 初始重试退避时间（秒）（0.1-5.0，默认：0.5）
### 最大重试退避时间（秒）（backoff-60.0，默认：5.0）
### 连接池关闭超时时间（秒）（1.0-30.0，默认：5.0）
# POSTGRES_CONNECTION_RETRIES=3
# POSTGRES_CONNECTION_RETRY_BACKOFF=0.5
# POSTGRES_CONNECTION_RETRY_BACKOFF_MAX=5.0
# POSTGRES_POOL_CLOSE_TIMEOUT=5.0

### PostgreSQL SSL 配置（可选）
# POSTGRES_SSL_MODE=require
# POSTGRES_SSL_CERT=/path/to/client-cert.pem
# POSTGRES_SSL_KEY=/path/to/client-key.pem
# POSTGRES_SSL_ROOT_CERT=/path/to/ca-cert.pem
# POSTGRES_SSL_CRL=/path/to/crl.pem

### PostgreSQL 服务器设置（用于 Supabase Supavisor）
### 使用此选项向 PostgreSQL 连接字符串传递额外选项。
### 对于 Supabase，您可能需要这样设置：
# POSTGRES_SERVER_SETTINGS="options=reference%3D[project-ref]"

### 默认为 100，设置为 0 以禁用
# POSTGRES_STATEMENT_CACHE_SIZE=100

### Neo4j 配置 host.docker.internal 
NEO4J_URI=bolt://host.docker.internal:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD='neo4j_password'
NEO4J_DATABASE=neo4j
NEO4J_MAX_CONNECTION_POOL_SIZE=100
NEO4J_CONNECTION_TIMEOUT=30
NEO4J_CONNECTION_ACQUISITION_TIMEOUT=30
NEO4J_MAX_TRANSACTION_RETRY_TIME=30
NEO4J_MAX_CONNECTION_LIFETIME=300
NEO4J_LIVENESS_CHECK_TIMEOUT=30
NEO4J_KEEP_ALIVE=true
### 不应设置特定于数据库的工作空间，仅保留以兼容
### NEO4J_WORKSPACE=forced_workspace_name

### MongoDB 配置
MONGO_URI=mongodb://root:root@localhost:27017/
#MONGO_URI=mongodb+srv://xxxx
MONGO_DATABASE=LightRAG
# MONGODB_WORKSPACE=forced_workspace_name

### Milvus 配置
MILVUS_URI=http://localhost:19530
MILVUS_DB_NAME=lightrag
# MILVUS_USER=root
# MILVUS_PASSWORD=your_password
# MILVUS_TOKEN=your_token
### 不应设置特定于数据库的工作空间，仅保留以兼容
### MILVUS_WORKSPACE=forced_workspace_name

### Qdrant
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=your-api-key
### 不应设置特定于数据库的工作空间，仅保留以兼容
### QDRANT_WORKSPACE=forced_workspace_name

### Redis
REDIS_URI=redis://localhost:6379
REDIS_SOCKET_TIMEOUT=30
REDIS_CONNECT_TIMEOUT=10
REDIS_MAX_CONNECTIONS=100
REDIS_RETRY_ATTEMPTS=3
### 不应设置特定于数据库的工作空间，仅保留以兼容
### REDIS_WORKSPACE=forced_workspace_name

### Memgraph 配置
MEMGRAPH_URI=bolt://localhost:7687
MEMGRAPH_USERNAME=
MEMGRAPH_PASSWORD=
MEMGRAPH_DATABASE=memgraph
### 不应设置特定于数据库的工作空间，仅保留以兼容
### MEMGRAPH_WORKSPACE=forced_workspace_name

###########################################################
### Langfuse 可观测性配置
### 仅适用于 OpenAI 兼容 API 提供的 LLM
### 安装方式: pip install lightrag-hku[observability]
### 注册地址: https://cloud.langfuse.com 或自托管
###########################################################
# LANGFUSE_SECRET_KEY=""
# LANGFUSE_PUBLIC_KEY=""
# LANGFUSE_HOST="https://cloud.langfuse.com"  # 或您的自托管实例地址
# LANGFUSE_ENABLE_TRACE=true

############################
### 评估配置
############################
### RAGAS 评估模型（用于 RAG 质量评估）
### ⚠️ 重要：LLM 和 Embedding 端点必须是 OpenAI 兼容的
### 默认使用 OpenAI 模型进行评估

### 评估用 LLM 配置
# EVAL_LLM_MODEL=gpt-4o-mini
### LLM 评估的 API 密钥（如果未设置则回退到 OPENAI_API_KEY）
# EVAL_LLM_BINDING_API_KEY=your_api_key
### LLM 评估的自定义 OpenAI 兼容端点（可选）
# EVAL_LLM_BINDING_HOST=https://api.openai.com/v1

### 评估用 Embedding 配置
# EVAL_EMBEDDING_MODEL=text-embedding-3-large
### embeddings 的 API 密钥（回退顺序: EVAL_LLM_BINDING_API_KEY -> OPENAI_API_KEY）
# EVAL_EMBEDDING_BINDING_API_KEY=your_embedding_api_key
### embeddings 的自定义 OpenAI 兼容端点（回退到 EVAL_LLM_BINDING_HOST）
# EVAL_EMBEDDING_BINDING_HOST=https://api.openai.com/v1

### 性能调优
### 并发测试用例评估数量
### 较低的值减少 API 速率限制问题，但增加评估时间
# EVAL_MAX_CONCURRENT=2
### LightRAG 的 TOP_K 查询参数（默认：10）
### 从知识图谱中检索的实体或关系数量
# EVAL_QUERY_TOP_K=10
### 评估的 LLM 请求重试和超时设置
# EVAL_LLM_MAX_RETRIES=5
# EVAL_LLM_TIMEOUT=180
